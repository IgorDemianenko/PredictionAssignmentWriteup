---
title: "Peer-graded Assignment: Prediction Assignment Writeup"
author: "Srilakshmi Uppalapati"
date: "October 27, 2018"
output: html_document
---

### Introduction and Goal

This is for the project assignment of the Pratical Machine Learning Course hosted in the Coursera platform. Our goal is to construct an algorithm to predict, based on kinetic parameters given
by a range of body sensors, if an individual performed a weight lifting exercise correctly. 

One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

In this project, Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes.

More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

### Data

The training data for this project came from here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data came from here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har. the Human Activity Recognition (HAR) group of the Rio de Janeiro PUC university have been very generous in allowing their data to be used for this kind of assignment.

###  Getting and reading the data sets

```{r}
# Download data
trainingUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testingUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(trainingUrl, destfile = "./training.csv", method = "curl")
download.file(testingUrl, destfile = "./testing.csv", method = "curl")

# Read data
training <- read.csv("./training.csv", na.strings = c("NA", "#DIV/0!"))
testing <- read.csv("./testing.csv", na.strings = c("NA", "#DIV/0!"))
```

### Looking at the data

```{r}
library(dplyr)
View(training)
View(testing)
```

We can see that in the training set we have 19622 observations of 160 variables and that in the testing set we have 20 observations of 160 variables. Many of that variables (columns) have a NAs and the first seven columns are pf little interest to prediction.

### Cleaning data

```{r}
# Remove variables in the training set with many NAs 
goodCol <- colSums(is.na(training)) < 1900
myTraining <- training[ , goodCol][ , ]

# Remove the same columns in the test set
myTesting <- testing[ , goodCol][ , ]

# Remove the first seven columns in both sets
myTraining <- myTraining[ , -(1:7)]
myTesting <- myTesting[ , -(1:7)] 
View(myTraining)
View(myTesting)
```

Now we have 19622 observations of 53 variables (training) and 20 observations of 53 variables (testing).

### Subsetting the training data

In building our model, for a cross validation objective, we subset our training data to a real training set and a test set.

```{r}
# Create inTraining and inTesting
library(caret)
set.seed(4848)
inTrain <- createDataPartition(y = myTraining$classe, p = 0.75, list = FALSE)
inTraining <- myTraining[inTrain, ]
inTesting <- myTraining[-inTrain, ]
```

### Building the model

Tree methods were tried: gradient boosting with "gbm", random forests with "rf" and random forests using the randomForest() functiom. The first two revealed themselves to be painfully slow, so they were disregarded and randomForest was choosed to training, tunning and testing.

```{r}
# Train with randomForest
library(randomForest)
set.seed(555)
rfGrid <-  expand.grid(interaction.depth = c(1, 5, 9),
                        n.trees = (1:30)*50,
                        shrinkage = 0.1)
modelFit <- randomForest(classe ~., data = inTraining, tuneGrid = rfGrid) 
print(modelFit)
plot(modelFit)
```

This model looked promissing, with very low classification errors in all classes, and a Out of the Box (OOB) error estimate that descends swiftly to near 0, as we can see in the plot above.

### Cross validation
 
```{r}
# Test "out of sample"
predictions <- predict(modelFit, newdata = inTesting)
confusionMatrix(predictions, inTesting$classe)
```

The model passed the test, with a global accuracy of 0.9988, a kappa of 0.9985 and with near perfect
sensivity and specificity for all classes.

### Final validation with results for submission

```{r}
# Test validation sample
answers <- predict(modelFit, newdata = myTesting, type = "response")
print(answers)
```

### Conclusion

All the 20 answers were validated as correct at the PML project submission page. 


